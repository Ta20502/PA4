import streamlit as st
import pandas as pd
import json
import io
from google import genai
from google.genai.errors import APIError

# ==============================================================================
# 0. ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö (Page Configuration)
# ==============================================================================

st.set_page_config(
    page_title="üì∞ Content Analyzer",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==============================================================================
# 1. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á System Prompt
# ==============================================================================

def create_system_prompt(n: int) -> str:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á System Prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå JSON ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î"""
    return f"""
You are an expert Content Analyzer and Linguist. Your task is to analyze the provided NEWS ARTICLE or TEXT.
You must perform four major tasks:
1.  **Summarize** the article briefly in 2-3 sentences.
2.  **Analyze the Tone (Sentiment)** of the article (e.g., Positive, Negative, Neutral, Informative).
3.  **Calculate the Frequency** of the {n} most important, non-stop-word nouns and verbs in the text.
4.  **Assess Readability** and suggest a reader level (e.g., High School, College, General Public).

Return the result *strictly* in a valid JSON object with the following three main keys:
-   'analysis_summary': A single object containing the summary and general analysis.
    -   'summary_text': The 2-3 sentence summary.
    -   'tone_analysis': The overall sentiment (e.g., Positive, Negative, Neutral).
    -   'readability_level': The suggested reader level.
-   'keyword_frequency': A JSON array of the {n} most important keywords.
    -   Each element in this array must be an object with keys: 'keyword', 'frequency_count', and 'part_of_speech'.

DO NOT include any introductory or concluding text outside the JSON object.
"""

# ==============================================================================
# 2. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Gemini API
# ==============================================================================

def get_gemini_response(api_key: str, system_prompt: str, user_text: str) -> str | None:
    """‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Google Gemini API ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô JSON"""
    if not api_key:
        st.error("‚ùå ‡πÇ‡∏õ‡∏£‡∏î‡πÉ‡∏™‡πà API Key ‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á 'API key' ‡∏î‡πâ‡∏≤‡∏ô‡∏ã‡πâ‡∏≤‡∏¢‡∏Å‡πà‡∏≠‡∏ô")
        return None

    try:
        client = genai.Client(api_key=api_key)
        model = "gemini-2.5-flash"
        
        response = client.models.generate_content(
            model=model,
            contents=[system_prompt, f"ARTICLE TEXT:\n\n{user_text}"],
            config={"response_mime_type": "application/json"} # ‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô JSON
        )
        
        return response.text

    except APIError as e:
        st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ API: ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API Key ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ({e})")
        return None
    except Exception as e:
        st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î: {e}")
        return None

# ==============================================================================
# 3. ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Streamlit (Main App)
# ==============================================================================

# --- 3.1 Sidebar ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö API Key ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ---
with st.sidebar:
    st.title("‚öôÔ∏è ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏£‡∏∞‡∏ö‡∏ö")
    
    # API Key Input
    user_api_key = st.text_input(
        "**Google AI API Key**", 
        type="password",
        help="‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà API Key ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏à‡∏≤‡∏Å Google AI Studio"
    )

    st.markdown("---")
    st.title("üî¢ ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå")
    
    # Slider ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Top keyword
    top_n_keywords = st.slider(
        "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏Ñ‡πç‡∏≤‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡πç‡∏≤‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• (Top N)",
        min_value=10,
        max_value=50,
        value=10, # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        step=5,
    )
    st.info(f"‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç {top_n_keywords} ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö")

# --- 3.2 Main Content Area ---

st.title('üì∞ Content Analyzer: ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡∏î‡πâ‡∏ß‡∏¢ AI')
st.markdown('‡∏õ‡πâ‡∏≠‡∏ô **‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏Ç‡πà‡∏≤‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°** ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö **‡∏™‡∏£‡∏∏‡∏õ ‡πÇ‡∏ó‡∏ô ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**')

# Input Text Area
article_text = st.text_area(
    "‡∏õ‡πâ‡∏≠‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏Ç‡πà‡∏≤‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà:",
    value="Large language models (LLMs) are deep learning models trained on vast amounts of text data. They can understand, summarize, and generate human-like text, making them revolutionary tools for various NLP applications. The development of LLMs requires immense computational resources, particularly high-end GPUs. Despite their power, LLMs still face challenges related to factual accuracy and ethical bias.",
    height=250,
    placeholder="‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏õ‡πâ‡∏≠‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏´‡∏£‡∏∑‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô"
)

# Submit Button
if st.button('üöÄ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤'):
    
    # 3.3 ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    if not user_api_key:
        st.error("‚ùå ‡πÇ‡∏õ‡∏£‡∏î‡πÉ‡∏™‡πà **API Key** ‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á 'API key' ‡∏î‡πâ‡∏≤‡∏ô‡∏ã‡πâ‡∏≤‡∏¢‡∏Å‡πà‡∏≠‡∏ô")
    elif not article_text or article_text.strip() == "":
        st.error("‚ùå ‡πÇ‡∏õ‡∏£‡∏î‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå")
    else:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á SYSTEM_PROMPT ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤ N ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        current_system_prompt = create_system_prompt(top_n_keywords)
        
        # 3.4 ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ API ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        with st.spinner("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏î‡πâ‡∏ß‡∏¢ Gemini..."):
            json_response_text = get_gemini_response(
                api_key=user_api_key,
                system_prompt=current_system_prompt,
                user_text=article_text
            )

        if json_response_text:
            try:
                # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå JSON ‡πÄ‡∏õ‡πá‡∏ô Python Dictionary
                result = json.loads(json_response_text)
                
                analysis_summary = result.get('analysis_summary', {})
                keyword_frequency = result.get('keyword_frequency', [])
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏µ‡∏¢‡πå‡∏´‡∏•‡∏±‡∏Å
                if not analysis_summary or not keyword_frequency:
                    st.warning("‚ö†Ô∏è ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå JSON ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå: ‡∏≠‡∏≤‡∏à‡∏Ç‡∏≤‡∏î 'analysis_summary' ‡∏´‡∏£‡∏∑‡∏≠ 'keyword_frequency'")
                    st.code(json_response_text)
                    raise ValueError("JSON Incomplete") 

                st.success("‚úÖ ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå")
                
                # --- 3.5 ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ---
                
                ## 1. ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏ó‡∏ô (Summary & Tone)
                st.header("1. ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°")
                
                summary_df = pd.DataFrame({
                    "Summary": [analysis_summary.get('summary_text', 'N/A')],
                    "Tone Analysis": [analysis_summary.get('tone_analysis', 'N/A')],
                    "Readability Level": [analysis_summary.get('readability_level', 'N/A')]
                })
                
                st.dataframe(
                    summary_df.T.rename(columns={0: "‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå"}), 
                    use_container_width=True, 
                    height=200 # ‡πÉ‡∏ä‡πâ Transpose ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏î‡∏π‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
                )
                
                st.markdown("---")
                
                ## 2. ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Keyword Frequency)
                st.header(f"2. ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Top {top_n_keywords} ‡∏Ñ‡∏≥)")
                
                frequency_df = pd.DataFrame(keyword_frequency)
                frequency_df.columns = ['‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è', '‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏û‡∏π‡∏î (POS)']
                frequency_df.insert(0, '‡∏•‡∏≥‡∏î‡∏±‡∏ö', range(1, 1 + len(frequency_df)))
                
                st.dataframe(
                    frequency_df, 
                    hide_index=True, 
                    use_container_width=True
                ) 
                
                # --- 3.6 ‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ---
                st.markdown("---")
                st.header("3. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")

                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡∏°‡∏µ 2 Sheets
                excel_buffer = io.BytesIO()
                with pd.ExcelWriter(excel_buffer) as writer:
                    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Summary (‡πÉ‡∏ä‡πâ DataFrame ‡∏ó‡∏µ‡πà Transpose ‡πÅ‡∏•‡πâ‡∏ß)
                    summary_df.T.rename(columns={0: "‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå"}).to_excel(writer, sheet_name='Summary_Analysis', header=True)
                    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Frequency
                    frequency_df.to_excel(writer, sheet_name='Keyword_Frequency', index=False) 
                excel_buffer.seek(0)
                
                st.download_button(
                    label="‚¨áÔ∏è ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel",
                    data=excel_buffer,
                    file_name='content_analysis_report.xlsx',
                    mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                    key='download_excel'
                )
                
            except (json.JSONDecodeError, ValueError) as e:
                # Catch JSON Decode Error ‡πÅ‡∏•‡∏∞ Value Error
                st.error(f"‚ùå Error: AI ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏´‡∏£‡∏∑‡∏≠ JSON ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå")
                st.markdown("**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏¥‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö:**")
                st.code(json_response_text)
            except Exception as e:

                st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")
