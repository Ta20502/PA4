import streamlit as st
import pandas as pd
import json
import io
from google import genai
from google.genai.errors import APIError

# ==============================================================================
# 0. ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö (Page Configuration)
# ==============================================================================

st.set_page_config(
    page_title="üì∞ Content Analyzer",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==============================================================================
# 1. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á System Prompt
# ==============================================================================

def create_system_prompt(n: int, summary_language: str) -> str:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á System Prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå JSON ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î"""
    if summary_language == "Thai":
        summary_instruction = "Summarize the article briefly in 2-3 sentences. **The summary must be in Thai.**"
    elif summary_language == "English":
        summary_instruction = "Summarize the article briefly in 2-3 sentences. **The summary must be in English.**"
    return f"""
You are an expert Content Analyzer and Linguist. Your task is to analyze the provided NEWS ARTICLE or TEXT.
You must perform four major tasks:
1.  **Summarize** the article briefly in 2-3 sentences.
2.  **Analyze the Tone (Sentiment)** of the article (e.g., Positive, Negative, Neutral, Informative).
3.  **Calculate the Frequency** of the {n} most important, non-stop-word nouns and verbs in the text.
4.  **Assess Readability** and suggest a reader level (e.g., High School, College, General Public).

Return the result *strictly* in a valid JSON object with the following three main keys:
-   'analysis_summary': A single object containing the summary and general analysis.
    -   'summary_text': The 2-3 sentence summary.
    -   'tone_analysis': The overall sentiment (e.g., Positive, Negative, Neutral).
    -   'readability_level': The suggested reader level.
-   'keyword_frequency': A JSON array of the {n} most important keywords.
    -   Each element in this array must be an object with keys: 'keyword', 'frequency_count', and 'part_of_speech'.

DO NOT include any introductory or concluding text outside the JSON object.
"""

# ==============================================================================
# 2. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Gemini API
# ==============================================================================

def get_gemini_response(api_key: str, system_prompt: str, user_text: str) -> str | None:
    """‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Google Gemini API ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô JSON"""
    if not api_key:
        st.error("‚ùå ‡πÇ‡∏õ‡∏£‡∏î‡πÉ‡∏™‡πà API Key ‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á 'API key' ‡∏î‡πâ‡∏≤‡∏ô‡∏ã‡πâ‡∏≤‡∏¢‡∏Å‡πà‡∏≠‡∏ô")
        return None

    try:
        client = genai.Client(api_key=api_key)
        model = "gemini-2.5-flash"
        
        response = client.models.generate_content(
            model=model,
            contents=[system_prompt, f"ARTICLE TEXT:\n\n{user_text}"],
            config={"response_mime_type": "application/json"} # ‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô JSON
        )
        
        return response.text

    except APIError as e:
        st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ API: ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API Key ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ({e})")
        return None
    except Exception as e:
        st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î: {e}")
        return None

# ==============================================================================
# 3. ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Streamlit (Main App)
# ==============================================================================

# --- 3.1 Sidebar ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö API Key ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ---
with st.sidebar:
    st.title("‚öôÔ∏è ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏£‡∏∞‡∏ö‡∏ö")
    
    # API Key Input
    user_api_key = st.text_input(
        "**API Key**", 
        type="password",
        help="‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà API Key ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì"
    )

    st.markdown("---")
    st.title("üî¢ ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå")
    
    # Slider ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Top keyword
    top_n_keywords = st.slider(
        "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏Ñ‡πç‡∏≤‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡πç‡∏≤‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•",
        min_value=10,
        max_value=50,
        value=10, # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        step=5,
    )
    st.info(f"‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç {top_n_keywords} ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö")

    st.markdown("---")
    st.title("üó£Ô∏è ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏†‡∏≤‡∏©‡∏≤")
    
    # **üî• NEW: Select box ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ**
    summary_language = st.selectbox(
        "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö **‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ**",
        options=["English", "Thai", "Original (‡∏ï‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡∏Ç‡∏≠‡∏á‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°)"],
        index=0, # English ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        help="‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ (Summary Text) ‡πÇ‡∏î‡∏¢ AI"
    )
    # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô prompt
    if "Thai" in summary_language:
        lang_code = "Thai"
    elif "English" in summary_language:
        lang_code = "English"

# --- 3.2 Main Content Area ---

st.title('üì∞ Content Analyzer: ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å')
st.markdown('‡∏õ‡πâ‡∏≠‡∏ô **‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏Ç‡πà‡∏≤‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°** ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠ **‡∏™‡∏£‡∏∏‡∏õ ‡πÇ‡∏ó‡∏ô ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**')

# Input Text Area
article_text = st.text_area(
    "‡∏õ‡πâ‡∏≠‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏Ç‡πà‡∏≤‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà:",
    value="Large language models (LLMs) are deep learning models trained on vast amounts of text data. They can understand, summarize, and generate human-like text, making them revolutionary tools for various NLP applications. The development of LLMs requires immense computational resources, particularly high-end GPUs. Despite their power, LLMs still face challenges related to factual accuracy and ethical bias.",
    height=250,
    placeholder="‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏õ‡πâ‡∏≠‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏´‡∏£‡∏∑‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô"
)

# Submit Button
if st.button('üöÄ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤'):
    
    # 3.3 ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    if not user_api_key:
        st.error("‚ùå ‡πÇ‡∏õ‡∏£‡∏î‡πÉ‡∏™‡πà **API Key** ‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á 'API key' ‡∏î‡πâ‡∏≤‡∏ô‡∏ã‡πâ‡∏≤‡∏¢‡∏Å‡πà‡∏≠‡∏ô")
    elif not article_text or article_text.strip() == "":
        st.error("‚ùå ‡πÇ‡∏õ‡∏£‡∏î‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå")
    else:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á SYSTEM_PROMPT ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤ N ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        current_system_prompt = create_system_prompt(top_n_keywords, lang_code)
        
        # 3.4 ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ API ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        with st.spinner("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì..."):
            json_response_text = get_gemini_response(
                api_key=user_api_key,
                system_prompt=current_system_prompt,
                user_text=article_text
            )

        if json_response_text:
            try:
                # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå JSON ‡πÄ‡∏õ‡πá‡∏ô Python Dictionary
                result = json.loads(json_response_text)
                
                analysis_summary = result.get('analysis_summary', {})
                keyword_frequency = result.get('keyword_frequency', [])
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏µ‡∏¢‡πå‡∏´‡∏•‡∏±‡∏Å
                if not analysis_summary or not keyword_frequency:
                    st.warning("‚ö†Ô∏è ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå JSON ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå: ‡∏≠‡∏≤‡∏à‡∏Ç‡∏≤‡∏î 'analysis_summary' ‡∏´‡∏£‡∏∑‡∏≠ 'keyword_frequency'")
                    st.code(json_response_text)
                    raise ValueError("JSON Incomplete") 

                st.success("‚úÖ ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå")
                
                # --- 3.5 ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ---
                
                ## 1. ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏ó‡∏ô (Summary & Tone)
                st.header("1. ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°")
                st.markdown("‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ä‡πà‡∏≠‡∏á‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ç‡∏¢‡∏≤‡∏¢‡∏î‡∏π‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
                
                summary_df = pd.DataFrame({
                    "Summary": [analysis_summary.get('summary_text', 'N/A')],
                    "Tone Analysis": [analysis_summary.get('tone_analysis', 'N/A')],
                    "Readability Level": [analysis_summary.get('readability_level', 'N/A')]
                })
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏ó‡∏µ‡πà Transpose ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
                summary_df_transposed = summary_df.T.rename(columns={0: "‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå"})
                
                st.dataframe(
                    summary_df_transposed, 
                    use_container_width=True, 
                    height=200 # ‡πÉ‡∏ä‡πâ Transpose ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏î‡∏π‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
                )
                
                st.markdown("---")
                
                ## 2. ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Keyword Frequency)
                st.header(f"2. ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Top {top_n_keywords} ‡∏Ñ‡∏≥)")
                
                frequency_df = pd.DataFrame(keyword_frequency)
                frequency_df.columns = ['‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è', '‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏û‡∏π‡∏î (POS)']
                frequency_df.insert(0, '‡∏•‡∏≥‡∏î‡∏±‡∏ö', range(1, 1 + len(frequency_df)))

                st.dataframe(
                    frequency_df, 
                    hide_index=True, 
                    use_container_width=True
                ) 

                st.header("3. ‡πÅ‡∏ú‡∏ô‡∏†‡∏π‡∏°‡∏¥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå")

                # ‡πÉ‡∏ä‡πâ st.bar_chart ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå
                # ‡πÇ‡∏î‡∏¢‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡∏ô X ‡πÄ‡∏õ‡πá‡∏ô '‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå' ‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡∏ô Y ‡πÄ‡∏õ‡πá‡∏ô '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è'
                st.bar_chart(
                    frequency_df.set_index('‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå')[['‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è']], 
                    use_container_width=True
                )
                
                # --- 3.6 ‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå  ---
                st.markdown("---")
                st.header("3. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")
                
                # 1. Download Summary CSV (‡πÉ‡∏ä‡πâ summary_df_transposed ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß)
                summary_csv = summary_df_transposed.to_csv(index=True, encoding='utf-8')
                st.download_button(
                    label="‚¨áÔ∏è ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå (CSV)",
                    data=summary_csv,
                    file_name='summary_analysis.csv',
                    mime='text/csv',
                    key='download_summary_csv'
                )

                # 2. Download Frequency CSV 
                frequency_csv = frequency_df.to_csv(index=False, encoding='utf-8')
                st.download_button(
                    label="‚¨áÔ∏è ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå (CSV)",
                    data=frequency_csv,
                    file_name='keyword_frequency.csv',
                    mime='text/csv',
                    key='download_frequency_csv'
                )
                
            except (json.JSONDecodeError, ValueError) as e:
                # Catch JSON Decode Error ‡πÅ‡∏•‡∏∞ Value Error
                st.error(f"‚ùå Error: AI ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏´‡∏£‡∏∑‡∏≠ JSON ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå: {e}")
                st.markdown("**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏¥‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö:**")
                st.code(json_response_text)
            except Exception as e:
                st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")






